[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bibhabasu Blogs",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nExponential Moving Average (EMA) in Weight Updates\n\n\n\n\n\n\npytorch\n\n\ncode\n\n\n\n\n\n\n\n\n\nMay 28, 2023\n\n\nBibhabasu Mohapatra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Understanding_ema/index.html",
    "href": "posts/Understanding_ema/index.html",
    "title": "Exponential Moving Average (EMA) in Weight Updates",
    "section": "",
    "text": "title: Exponential Moving Average (EMA) in Weight Updates author: “Bibhabasu Mohapatra” date: “2023-05-28” categories: [pytorch, code] image: “image.jpg”\n\n\nEMA (Exponential Moving Average) is an incredibly useful concept that finds application in various scenarios:\n\nWeight Updates: EMA is used for updating model weights while retaining a historical record of previous weights. This enables the model to blend new information with past knowledge effectively.\nSelf-Supervised Learning: EMA is commonly employed in Self-Supervised Learning setups. The weights obtained from Self-Supervised Learning are often utilized for downstream tasks like classification and segmentation.\n\n\n\nClarification on EMA’s Impact\n\nInitially, there was a misconception that the EMA process directly impacts the ongoing training of model weights. However, this is not the case. In reality, the EMA process involves the creation of a duplicated set of weights. These duplicate weights are updated alongside the primary training process, and the updated weights are subsequently leveraged for validation purposes. As a result, the overall training procedure remains unaffected by the EMA process.\n\n::: {#4619f07f .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-08-14T15:14:27.136587Z”,“iopub.status.busy”:“2023-08-14T15:14:27.136194Z”,“iopub.status.idle”:“2023-08-14T15:14:30.874853Z”,“shell.execute_reply”:“2023-08-14T15:14:30.873447Z”}’ papermill=‘{“duration”:3.746175,“end_time”:“2023-08-14T15:14:30.877588”,“exception”:false,“start_time”:“2023-08-14T15:14:27.131413”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\nimport torch.nn as nn\nimport torch\nfrom copy import deepcopy\n\nclass Model(nn.Module):\n    def __init__(self,):\n        super().__init__()\n\n        self.layer = nn.Linear(3,3,bias=False)\n\n    def forward(self,x):\n        return self.layer(x)\n    \n@torch.no_grad()\ndef init_weights_1(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\n@torch.no_grad()\ndef init_weights_2(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(2.0)\n        print(m.weight)\n:::\n\nteacher = Model().apply(init_weights_1) ## TO BE UPDATED\nstudent = Model().apply(init_weights_2) ## From Training\n\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\nLinear(in_features=3, out_features=3, bias=False)\nParameter containing:\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], requires_grad=True)\nModel(\n  (layer): Linear(in_features=3, out_features=3, bias=False)\n)\n\n\n\n@torch.no_grad()\ndef ema_model(teacher_model, student_model,decay=0.5):\n    teacher_model.eval()\n\n    for teacher_wt,student_wt in zip(teacher_model.state_dict().values(),student_model.state_dict().values()):\n        teacher_wt.copy_(teacher_wt*decay + student_wt*(1-decay))\n\n\nema_model(teacher,student)\n\n\nteacher.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000],\n                      [1.5000, 1.5000, 1.5000]]))])\n\n\n\nstudent.state_dict()\n\nOrderedDict([('layer.weight',\n              tensor([[2., 2., 2.],\n                      [2., 2., 2.],\n                      [2., 2., 2.]]))])"
  }
]